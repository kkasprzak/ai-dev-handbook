# TDD Flow with Claude Code: The @tdd/ Command System for AI-Assisted Development Control

_September 3, 2025_

## Introduction

After several months of working with Claude Code in Solidity and PHP projects, I developed a set of `@tdd/` commands that help control AI-assisted development. This post describes the evolution of my workflow and the solutions to problems encountered during pair programming with AI.

## The Problem: Controlling AI in TDD

While working with Claude Code, I identified several issues with adhering to TDD principles:

- **Writes production code before tests**
- **Implements multiple features at once** instead of a step-by-step approach
- **Forgets to refactor** after tests turn green
- **Loses context** between work sessions

From a July 2025 retrospective:

> "Claude had trouble smoothly switching between the acceptance test and the unit test. Once he wrote the acceptance test, he immediately started implementing the logic in the class and forgot that he should also create a test case in the unit test first."

## The Solution: The @tdd/ Command System

The current structure of @tdd/ commands is the result of iterative work—developed through a series of experiments, observations, and adaptations. It was not a one-time act of creation, but a process of continuous improvement: I experiment, analyze results, and make changes. I currently use four main commands, but this set and their roles may change as the system evolves and new insights emerge.

### 1. `@tdd/rules/remind` – Role Reminder

**Purpose**: Refresh the context of the implementation process and roles in the session
**When to use**: At the start of a session or when Claude loses focus on TDD

```markdown
Please read CLAUDE.md and remind ourselves what our implementation process is
and what our role is in this session
```

### 2. `@tdd/rules/violations` – Rule Violation Control

**Purpose**: Identify and correct TDD rule violations
**When to use**: When Claude writes code without tests or breaks the Red-Green-Refactor cycle

This command enforces analysis of:

- Mandatory TDD steps: Write ONE failing test → Run → Minimal implementation → Run → Refactor
- Rule about ONE test at a time
- Prohibition against writing features without tests first
- Red-Green-Refactor cycle violations

### 3. `@tdd/session/save` – Context Saving

**Purpose**: Structured pair programming session notes that not only document progress but, above all, prepare a complete and concise context for later resumption of work. This command is closely linked to the "Resume Work" function—good notes minimize the cost of context switching and allow the Navigator to quickly recall where we left off. As a result, returning to a task after a break is much easier and more efficient.

It generates notes in the following format:

```markdown
# Session Notes - [Date/Time]

## Feature Summary

[2-3 sentence summary]

## Current Status

• [Status point 1]
• [Status point 2]

## Next Tasks

• [Task 1]
• [Task 2]

## Important Reminders

• [Critical notes]
```

### 4. `@tdd/session/resume` – Resume Work

**Purpose**: Help the Navigator (me) quickly regain context after a break. The human mind has natural limitations in switching between tasks—returning to an interrupted thread can be time-consuming and requires memory refresh. This command was created precisely to minimize the time needed to remember where I left off and to enable a smooth resumption of work without unnecessary searching for information.

It analyzes `.ai/session_notes.md` and suggests:

- What we were doing (1–2 sentences)
- Immediate next action (a specific step)
- Current Status (list of points)
- Next Tasks (upcoming tasks)
- Quick Start (commands to run)

## Key Workflow Principles

### AI-Human Pair Programming Model

**Navigator (Me)**: Strategic decisions, architecture, code review
**Driver (Claude)**: Writing code, executing commands, implementation

**Principle**: Claude asks for permission before each TDD step.

Example communication:

```
Driver: "Should I write a failing test for [specific behavior]?"
Navigator: "Yes, start with time-based trigger validation"
Driver: "Test created and failing. May I proceed to implement minimal solution?"
Navigator: "Yes, implement just the time check"
```

### TDD Process

Following the cycle:

1. **RED**: Write a failing test
2. **GREEN**: Minimal implementation to pass the test
3. **REFACTOR**: Improve the code while keeping tests green

### Quality Control

- xUnit Test Patterns as a foundation
- Business-focused test naming
- Creation Methods for clean setup
- Consistent patterns throughout the project

## Evolution and Learning

### Early Discoveries (July 2025)

> "Writing test cases and implementing based on the written test went well. Claude stuck well to the acceptance test and then the implementation."

But also the first problems:

> "Claude had trouble smoothly switching between the acceptance test and the unit test."

### System Development (August 2025)

Introduction of pair programming rules:

> "Pair programming works well where AI is the driver and I am the navigator. I have rules for this written in CLAUDE.md and the AI listens quite well."

### Current State (September 2025)

Additionally, I use:

- Output styles (Junior developer mentee, Code review terror)
- Specialized subagents (Product Owner, Smart Contract Auditor)
- Plan mode (`Shift+Tab`) for better planning
- Session continuity through structured notes

## Practical Benefits

### 1. **Process Consistency**

Repeatable TDD flow for various tasks.

### 2. **Maintaining Control**

Strategic decisions remain with the human.

### 3. **Quality Control**

Procedures prevent skipping tests or refactoring.

### 4. **Session Continuity**

Context saving and easy resumption of work.

### 5. **Versatility**

The system is used in PHP, Solidity, TypeScript/React.

## Key Insights from Retrospectives

### AI in New Areas

> "Switching to another technology with Claude Code is easier—I can start delivering value faster."

### Need for Balance with Independent Coding

> "Since AI writes code more and more often, I notice that I should also write on my own to maintain this skill."

### Plan Mode

> "I spend more time discussing the plan with Claude Code, and when I see the plan is solid, I switch to accept-on-edit mode so it writes the full implementation."

## Further Development

I am also testing:

- Specialized subagents (Product Owner, Code Reviewer)
- Output styles tailored to context (Learning, Code review terror)
- Tooling integration (Puppeteer MCP, PostgreSQL MCP)
- Session continuity improvements

## Development Plans: Beck-Style Note Command

One of my next goals is to introduce a dedicated command that will allow me to quickly and in a structured way record notes during work—just as Kent Beck does in "Test-Driven Development: By Example." I want to be able to easily add:

- TODO lists (things to do, next tests, refactorings)
- Observations and doubts arising during implementation
- Ideas for improvements or technical debt to be addressed later

Such a command will not only help me better manage context and action plans but also make it easier to resume work after a break and systematically organize the code.

In the spirit of Kent Beck's practices, I also plan to leave an unimplemented test case at the end of a session. This is a simple but effective way to remind myself where to start the next session—making it easier to return to work and not waste time finding the right place in the TDD cycle.

## Balancing AI Control – Inspired by Karpathy's Slider

Currently, my system for working with the AI assistant is based on full control—I keep the AI on a "short leash" through a strict TDD process and my own commands. This approach minimizes the risk of errors and maintains high code quality, but it comes at a cost: I do not fully leverage the AI's potential to speed up work. If I switched to "vibe coding," where the AI has more autonomy and generates solutions based on general guidelines, I could achieve greater speed, but at the cost of potential technical debt and loss of quality control.

Andrej Karpathy's observations about the "autonomy slider" capture this dilemma well—on one side, full control; on the other, full AI freedom. I am currently experimenting with different slider settings, looking for the golden mean that will allow me to effectively combine safety and quality with the speed and flexibility of working with AI. This is the direction of my further experiments with AI assistants in programming.

## Refactoring with AI – Challenges and Plans

I have repeatedly noticed that AI knows all the popular refactoring methods (e.g., split phase, extract method, replace temp with query, replace primitive with object), but does not apply them independently without explicit instruction. As I wrote in a retrospective:

> "The LLM, if you tell it what to do, will do it, but it can't connect the dots like a human. Interestingly, it knows these methods very well—whenever I ask, it explains them flawlessly. But it can't independently decide that it should use them in a given context."

Similarly:

> "It works well when I specifically name the refactoring I want it to perform. The assistant knows them all, so I don't have to explain step by step."

I am currently looking for ways to improve the refactoring process with AI. Perhaps new commands will appear (e.g., quickly marking places for later refactoring) or an extension of CLAUDE.md with project-specific refactoring patterns. I would like refactoring to become a more natural part of the AI workflow, not just a reaction to my commands.

## Summary

The `@tdd/` command system helps control AI-assisted development by structuring the TDD process. The main assumption is to keep human control over strategic decisions while using AI for implementation.

Effects:

- Control over the code remains with the human
- AI assists with implementation
- Procedures prevent skipping tests
- Ability to work in various technologies
- Session continuity

The system provides the structures needed for effective work with AI in TDD.
