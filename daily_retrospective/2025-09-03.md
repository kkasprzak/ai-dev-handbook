# AI Daily Retrospective - 2025-09-03

## **What new things did I test/apply today in the context of AI/systems?**

- I installed Puppeteer MCP and am testing how it can help me.
- I defined two output styles for Claude Code:
  - Junior developer mentee: Acts like a junior developer seeking mentorship—asks questions, learns by doing TDD, works in tiny steps.
  - Code review terror: Becomes the intimidating senior developer who questions everything and demands perfection.
- I want to see how Claude Code behaves with these output styles.
- I created a new Product Owner subagent that writes elegant user stories for my PRD.
- I created a new QA subagent to help developers identify implementation gaps, edge cases for automated tests, and security issues. I still need to test the collaboration between the AI QA and myself as a developer. Currently, I want it to generate a list of test cases for the functionality I’m implementing—no code, just test cases with short descriptions for context. Ideally, it would also generate a template for a Foundry test class.
- I created a command that fully automates all the steps I usually take when starting work on a new ticket. I check the ticket in Jira to understand it, then create a feature branch for the new functionality according to project conventions. Next, I run automated tests to ensure I’m starting from a clean state. Then I create a file with TDD session notes in a dedicated directory. Now Claude Code handles all of this, and I just watch it happen.
- I added two new commands for saving and removing technical debt in the current TDD session. This allows me to manage technical debt in a structured way during TDD, just as Kent Beck suggests—so I can note debt to address later after finishing the current implementation thread.

## **What specifically worked, and what was difficult?**

- It’s hard for me to assess whether the test cases generated by the AI QA are valuable. This is mainly because the functionality I’m working on involves integration with Chainlink VRF v2 and Chainlink Automation, which I’m not familiar with.

## **What observation/conclusion do I take from today?**

- The "intimidating senior developer" output style is good—it gives more valuable feedback than the regular style during code review.
- The selected output style also affects the subagent. Yesterday, when I launched the QA subagent with the "intimidating senior developer" output style, I received interesting but valuable feedback. I think with a "nice" AI, I wouldn’t have been called out so much. This is a great case for experimenting and comparing feedback from different output styles.
- It’s worth automating everything I notice I repeat more than once. Why do it manually if the assistant can do it for me—and do it well!

## **How can I use this learning tomorrow/in the future?**

- I still want to test this QA subagent because I see it also catches edge cases I hadn’t thought of. Thanks to this, the quality and security of the smart contract should be higher.
